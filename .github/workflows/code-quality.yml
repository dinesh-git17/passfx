# Code Quality Gate
# Enforces formatting, linting, compilation safety, attribution guards, and tests.
# Split into semantic jobs for clarity and parallel execution.
#
# Job Structure:
#   - code-quality: Formatting, linting, attribution (runs once on 3.11)
#   - core-security-tests: Critical security and core module validation
#   - ui-tests: Textual UI, screens, and app lifecycle tests
#   - utils-cli-tests: Utility functions, CLI, and edge case tests
#
# Performance tests (marked @pytest.mark.slow) are excluded from CI.
# They run via scheduled workflow or manual trigger.

name: Code Quality

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

# Cancel in-progress runs for the same PR or branch
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  # ==========================================================================
  # Job 1: Code Quality Checks (Formatting, Linting, Attribution)
  # ==========================================================================
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install black pylint isort pre-commit

      - name: Check formatting (black)
        run: black --check --diff passfx/

      - name: Check import sorting (isort)
        run: isort --check-only --diff --profile black passfx/

      - name: Lint (pylint)
        run: pylint passfx/ --rcfile=.pylintrc --fail-under=10.0

      - name: Compilation check
        run: find passfx/ -name "*.py" -type f | xargs python -m py_compile

      - name: Attribution guard
        run: |
          python scripts/attribution_guard.py $(find . \
            -type f \( -name "*.py" -o -name "*.md" -o -name "*.txt" -o -name "*.yaml" -o -name "*.yml" -o -name "*.toml" \) \
            ! -path "./.git/*" \
            ! -path "./.venv/*" \
            ! -path "./venv/*" \
            ! -path "./__pycache__/*" \
            ! -path "./.pytest_cache/*" \
            ! -path "./.mypy_cache/*" \
            ! -path "./.ruff_cache/*" \
            ! -path "./dist/*" \
            ! -path "./build/*" \
            ! -path "./*.egg-info/*" \
            ! -name "CLAUDE.md" \
            2>/dev/null || true)

      - name: Pre-commit parity check
        run: pre-commit run --all-files

  # ==========================================================================
  # Job 2: Core & Security Tests
  # Critical security and data integrity validation
  # Includes: crypto, vault, models, integration, security, regression tests
  # ==========================================================================
  core-security-tests:
    name: Core & Security (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    needs: code-quality

    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10', '3.11']

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run core & security tests
        run: |
          pytest tests/unit/core tests/integration tests/security tests/regression \
            --cov=passfx.core \
            --cov-report=term-missing \
            --cov-report=xml:coverage-core.xml \
            --cov-fail-under=0 \
            -v \
            --tb=short \
            -ra

      - name: Test summary
        if: always()
        run: |
          echo "## Core & Security Tests (Python ${{ matrix.python-version }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Categories Executed:**" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/unit/core/\` - Crypto, Vault, Models (~950 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/integration/\` - Encryption round-trips (~50 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/security/\` - Threat model validation (~190 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/regression/\` - Security invariant locks (~100 tests)" >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.11'
        with:
          files: ./coverage-core.xml
          flags: core-security
          name: core-security-coverage
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # ==========================================================================
  # Job 3: UI & Screens Tests
  # Textual TUI, screen behavior, and app lifecycle validation
  # Includes: UI, screens, app lifecycle tests
  # ==========================================================================
  ui-tests:
    name: UI & Screens (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    needs: code-quality

    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10', '3.11']

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run UI & screens tests
        run: |
          pytest tests/ui tests/app tests/screens \
            --cov=passfx.screens \
            --cov-report=term-missing \
            --cov-report=xml:coverage-ui.xml \
            --cov-fail-under=0 \
            -v \
            --tb=short \
            -ra

      - name: Test summary
        if: always()
        run: |
          echo "## UI & Screens Tests (Python ${{ matrix.python-version }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Categories Executed:**" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/ui/\` - Login security, search state machine (~130 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/app/\` - App lifecycle management (~30 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/screens/\` - Credential screen behavior (~50 tests)" >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.11'
        with:
          files: ./coverage-ui.xml
          flags: ui-screens
          name: ui-screens-coverage
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # ==========================================================================
  # Job 4: Utilities & CLI Tests
  # Helper functions, CLI entry points, and edge case validation
  # Includes: utils, CLI, edge case tests
  # ==========================================================================
  utils-cli-tests:
    name: Utilities & CLI (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    needs: code-quality

    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10', '3.11']

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run utility & CLI tests
        run: |
          pytest tests/utils tests/cli tests/edge \
            --cov=passfx.utils \
            --cov-report=term-missing \
            --cov-report=xml:coverage-utils.xml \
            --cov-fail-under=0 \
            -v \
            --tb=short \
            -ra

      - name: Test summary
        if: always()
        run: |
          echo "## Utilities & CLI Tests (Python ${{ matrix.python-version }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Categories Executed:**" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/utils/\` - Password generator, strength, clipboard, I/O (~310 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/cli/\` - CLI entry point validation (~20 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- \`tests/edge/\` - Failure modes and edge cases (~100 tests)" >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.11'
        with:
          files: ./coverage-utils.xml
          flags: utils-cli
          name: utils-cli-coverage
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # ==========================================================================
  # Job 5: Performance Test Status
  # Documents that slow tests are intentionally excluded from CI
  # ==========================================================================
  performance-status:
    name: Performance Tests (Skipped)
    runs-on: ubuntu-latest
    needs: code-quality

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Report skipped performance tests
        run: |
          echo "## Performance Tests Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "⏭️ **Intentionally Skipped in CI**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Performance tests marked with \`@pytest.mark.slow\` are excluded from the" >> $GITHUB_STEP_SUMMARY
          echo "fast CI pipeline to maintain quick feedback for contributors." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**What's Skipped:**" >> $GITHUB_STEP_SUMMARY
          pytest tests/performance --collect-only -q 2>/dev/null | head -20 >> $GITHUB_STEP_SUMMARY || true
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**How to Run Locally:**" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`bash" >> $GITHUB_STEP_SUMMARY
          echo "pytest tests/performance --run-slow -v" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Scheduled Runs:**" >> $GITHUB_STEP_SUMMARY
          echo "Performance tests run weekly via the [Performance Tests workflow](../../actions/workflows/performance-tests.yml)." >> $GITHUB_STEP_SUMMARY

      - name: Create annotation
        run: |
          echo "::notice title=Performance Tests::Skipped in CI (5 slow tests). Run locally with: pytest tests/performance --run-slow"

  # ==========================================================================
  # Final Summary Job
  # Aggregates all test results and posts PR comment with mergeability verdict
  # ==========================================================================
  ci-summary:
    name: CI Summary
    runs-on: ubuntu-latest
    needs: [code-quality, core-security-tests, ui-tests, utils-cli-tests, performance-status]
    if: always()

    steps:
      - name: Determine job statuses
        id: status
        run: |
          # Map job results to display status
          get_status() {
            case "$1" in
              success) echo "✅ Passed" ;;
              failure) echo "❌ Failed" ;;
              cancelled) echo "⚪ Cancelled" ;;
              skipped) echo "⏭️ Skipped" ;;
              *) echo "❓ Unknown" ;;
            esac
          }

          # Collect all job results
          CODE_QUALITY="${{ needs.code-quality.result }}"
          CORE_SECURITY="${{ needs.core-security-tests.result }}"
          UI_TESTS="${{ needs.ui-tests.result }}"
          UTILS_CLI="${{ needs.utils-cli-tests.result }}"
          PERF_STATUS="${{ needs.performance-status.result }}"

          # Set status outputs
          echo "code_quality=$(get_status $CODE_QUALITY)" >> $GITHUB_OUTPUT
          echo "core_security=$(get_status $CORE_SECURITY)" >> $GITHUB_OUTPUT
          echo "ui_tests=$(get_status $UI_TESTS)" >> $GITHUB_OUTPUT
          echo "utils_cli=$(get_status $UTILS_CLI)" >> $GITHUB_OUTPUT
          echo "perf_status=$(get_status $PERF_STATUS)" >> $GITHUB_OUTPUT

          # Determine mergeability (all required jobs must pass)
          if [[ "$CODE_QUALITY" == "success" && \
                "$CORE_SECURITY" == "success" && \
                "$UI_TESTS" == "success" && \
                "$UTILS_CLI" == "success" ]]; then
            echo "mergeable=true" >> $GITHUB_OUTPUT
            echo "verdict=✅ **Mergeable** — All required checks passed." >> $GITHUB_OUTPUT
          else
            echo "mergeable=false" >> $GITHUB_OUTPUT

            # Build failure reason
            FAILURES=""
            [[ "$CODE_QUALITY" != "success" ]] && FAILURES="$FAILURES code quality,"
            [[ "$CORE_SECURITY" != "success" ]] && FAILURES="$FAILURES core/security tests,"
            [[ "$UI_TESTS" != "success" ]] && FAILURES="$FAILURES UI tests,"
            [[ "$UTILS_CLI" != "success" ]] && FAILURES="$FAILURES utility tests,"
            FAILURES="${FAILURES%,}"  # Remove trailing comma

            echo "verdict=❌ **Not Mergeable** — Failed:$FAILURES" >> $GITHUB_OUTPUT
          fi

      - name: Generate CI summary
        run: |
          echo "# PassFX CI Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status | Purpose |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality | ${{ steps.status.outputs.code_quality }} | Formatting, linting, attribution verification |" >> $GITHUB_STEP_SUMMARY
          echo "| Core & Security | ${{ steps.status.outputs.core_security }} | Encryption, vault storage, security invariants |" >> $GITHUB_STEP_SUMMARY
          echo "| UI & Screens | ${{ steps.status.outputs.ui_tests }} | Terminal UI, screen behavior, app lifecycle |" >> $GITHUB_STEP_SUMMARY
          echo "| Utilities & CLI | ${{ steps.status.outputs.utils_cli }} | Helper functions, CLI, edge case handling |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance | ${{ steps.status.outputs.perf_status }} | Benchmarks (skipped in CI, runs weekly) |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Merge Status:** ${{ steps.status.outputs.verdict }}" >> $GITHUB_STEP_SUMMARY

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
        run: |
          # Build comment body
          COMMENT_BODY=$(cat <<'COMMENT_EOF'
          <!-- PASSFX_CI_SUMMARY -->
          ## CI Summary

          | Job | Status | Purpose |
          |-----|--------|---------|
          | **Code Quality** | ${{ steps.status.outputs.code_quality }} | Formatting, linting, attribution verification |
          | **Core & Security** | ${{ steps.status.outputs.core_security }} | Encryption, vault storage, security invariants |
          | **UI & Screens** | ${{ steps.status.outputs.ui_tests }} | Terminal UI, screen behavior, app lifecycle |
          | **Utilities & CLI** | ${{ steps.status.outputs.utils_cli }} | Helper functions, CLI, edge case handling |
          | **Performance** | ${{ steps.status.outputs.perf_status }} | Benchmarks (skipped in CI, runs weekly) |

          ---

          **Merge Status:** ${{ steps.status.outputs.verdict }}

          <sub>
          Python versions tested: 3.10, 3.11 · Coverage: Phase 0 (0% threshold) · [View workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          </sub>
          COMMENT_EOF
          )

          # Find existing comment by marker
          EXISTING_COMMENT_ID=$(gh api \
            "repos/${{ github.repository }}/issues/${PR_NUMBER}/comments" \
            --jq '.[] | select(.body | contains("<!-- PASSFX_CI_SUMMARY -->")) | .id' \
            | head -1)

          if [[ -n "$EXISTING_COMMENT_ID" ]]; then
            # Update existing comment
            gh api \
              "repos/${{ github.repository }}/issues/comments/${EXISTING_COMMENT_ID}" \
              -X PATCH \
              -f body="$COMMENT_BODY"
            echo "Updated existing CI summary comment (ID: $EXISTING_COMMENT_ID)"
          else
            # Create new comment
            gh api \
              "repos/${{ github.repository }}/issues/${PR_NUMBER}/comments" \
              -f body="$COMMENT_BODY"
            echo "Created new CI summary comment"
          fi

      - name: Set exit status
        if: steps.status.outputs.mergeable != 'true'
        run: exit 1
